{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "from utils.atari_wrappers import wrap_dqn\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('Use GPU: {}'.format(use_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLAY_BUFFER_FILL_LEN = 10\n",
    "BATCH_SIZE = 16\n",
    "EPISODES = 100\n",
    "MAX_EPSILON_STEPS = 10000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.2\n",
    "SYNC_TARGET_NET_FREQ = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    '''\n",
    "    Replay memory to store states, actions, rewards, dones for batch sampling\n",
    "    '''\n",
    "    def __init__(self, capacity):\n",
    "        '''\n",
    "        :param capacity: replay memory capacity\n",
    "        '''\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    # e_t = (s_t,, a_t, r_t+1, s_t+1)\n",
    "    def add(self, state, action, reward, done, next_state):\n",
    "        '''\n",
    "        :param state: current state, atari_wrappers.LazyFrames object\n",
    "        :param action: action\n",
    "        :param reward: reward for the action\n",
    "        :param done: \"done\" flag is True when the episode finished\n",
    "        :param next_state: next state, atari_wrappers.LazyFrames object\n",
    "        '''\n",
    "        experience = (state, action, reward, done, next_state)\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''\n",
    "        Samples the data from the buffer of a desired size\n",
    "\n",
    "        :param batch_size: sample batch size\n",
    "        :return: batch of (states, actions, rewards, dones, next states).\n",
    "                 all are numpy arrays. states and next states have shape of\n",
    "                 (batch_size, frames, width, height), where frames = 4.\n",
    "                 actions, rewards and dones have shape of (batch_size,)\n",
    "        '''\n",
    "        if self.count() < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count())\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        state_batch = np.array([np.array(experience[0]) for experience in batch])\n",
    "        action_batch = np.array([experience[1] for experience in batch])\n",
    "        reward_batch = np.array([experience[2] for experience in batch])\n",
    "        done_batch = np.array([experience[3] for experience in batch])\n",
    "        next_state_batch = np.array([np.array(experience[4]) for experience in batch])\n",
    "\n",
    "        return state_batch, action_batch, reward_batch, done_batch, next_state_batch\n",
    "\n",
    "    def count(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Forward propogation\n",
    "\n",
    "        :param inputs: images. expected sshape is (batch_size, frames, width, height)\n",
    "        '''\n",
    "        out = self.relu(self.conv1(inputs))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.relu(self.conv3(out))\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.fc5(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongAgent:\n",
    "    def __init__(self, mode=None):\n",
    "        self.env = wrap_dqn(gym.make('PongDeterministic-v4'))\n",
    "        if mode == 'test':\n",
    "            self.env = Monitor(self.env, './video', force=True, video_callable=lambda episode_id: True)\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        self.dqn = DQN(self.num_actions)\n",
    "        self.target_dqn = DQN(self.num_actions)\n",
    "\n",
    "        if use_gpu:\n",
    "            self.dqn.cuda()\n",
    "            self.target_dqn.cuda()\n",
    "\n",
    "        self.buffer = ReplayMemory(1000)\n",
    "\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.optim = optim.SGD(self.dqn.parameters(), lr=0.01)\n",
    "\n",
    "        self.out_dir = './model'\n",
    "        self.writer = SummaryWriter()\n",
    "\n",
    "        if not os.path.exists(self.out_dir):\n",
    "            os.makedirs(self.out_dir)\n",
    "\n",
    "\n",
    "    def to_var(self, x):\n",
    "        x_var = Variable(x)\n",
    "        if use_gpu:\n",
    "            x_var = x_var.cuda()\n",
    "        return x_var\n",
    "\n",
    "\n",
    "    def predict_q_values(self, states):\n",
    "        states = self.to_var(torch.from_numpy(states).float())\n",
    "        actions = self.dqn(states)\n",
    "        return actions\n",
    "\n",
    "\n",
    "    def predict_q_target_values(self, states):\n",
    "        states = self.to_var(torch.from_numpy(states).float())\n",
    "        actions = self.target_dqn(states)\n",
    "        return actions\n",
    "\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        choice = np.random.choice([0, 1], p=(epsilon, (1 - epsilon)))\n",
    "\n",
    "        if choice == 0:\n",
    "            return np.random.choice(range(self.num_actions))\n",
    "        else:\n",
    "            state = np.expand_dims(state, 0)\n",
    "            actions = self.predict_q_values(state)\n",
    "            return np.argmax(actions.data.cpu().numpy())\n",
    "\n",
    "\n",
    "    def update(self, predicts, targets, actions):\n",
    "        targets = self.to_var(torch.unsqueeze(torch.from_numpy(targets).float(), -1))\n",
    "        actions = self.to_var(torch.unsqueeze(torch.from_numpy(actions).long(), -1))\n",
    "\n",
    "        affected_values = torch.gather(predicts, 1, actions)\n",
    "        loss = self.mse_loss(affected_values, targets)\n",
    "\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "\n",
    "    def get_epsilon(self, total_steps, max_epsilon_steps, epsilon_start, epsilon_final):\n",
    "        return max(epsilon_final, epsilon_start - total_steps / max_epsilon_steps)\n",
    "\n",
    "\n",
    "    def sync_target_network(self):\n",
    "        primary_params = list(self.dqn.parameters())\n",
    "        target_params = list(self.target_dqn.parameters())\n",
    "        for i in range(0, len(primary_params)):\n",
    "            target_params[i].data[:] = primary_params[i].data[:]\n",
    "\n",
    "\n",
    "    def calculate_q_targets(self, next_states, rewards, dones):\n",
    "        dones_mask = (dones == 1)\n",
    "\n",
    "        predicted_q_target_values = self.predict_q_target_values(next_states)\n",
    "\n",
    "        next_max_q_values = np.max(predicted_q_target_values.data.cpu().numpy(), axis=1)\n",
    "        next_max_q_values[dones_mask] = 0 # no next max Q values if the game is over\n",
    "        q_targets = rewards + self.gamma * next_max_q_values\n",
    "\n",
    "        return q_targets\n",
    "\n",
    "\n",
    "    def save_final_model(self):\n",
    "        filename = '{}/final_model.pth'.format(self.out_dir)\n",
    "        torch.save(self.dqn.state_dict(), filename)\n",
    "\n",
    "\n",
    "    def save_model_during_training(self, episode):\n",
    "        filename = '{}/current_model_{}.pth'.format(self.out_dir, episode)\n",
    "        torch.save(self.dqn.state_dict(), filename)\n",
    "\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        self.dqn.load_state_dict(torch.load(filename))\n",
    "        self.sync_target_network()\n",
    "\n",
    "\n",
    "    def play(self, episodes):\n",
    "        for i in range(1, episodes + 1):\n",
    "            done = False\n",
    "            state = self.env.reset()\n",
    "            while not done:\n",
    "                action = self.select_action(state, 0) # force to choose an action from the network\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                # self.env.render()\n",
    "\n",
    "\n",
    "    def close_env(self):\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "    def train(self, replay_buffer_fill_len, batch_size, episodes,\n",
    "              max_epsilon_steps, epsilon_start, epsilon_final, sync_target_net_freq):\n",
    "        start_time = time.time()\n",
    "        print('Start training at: '+ time.asctime(time.localtime(start_time)))\n",
    "\n",
    "        total_steps = 0\n",
    "        running_episode_reward = 0\n",
    "\n",
    "        # populate replay memory\n",
    "        print('Populating replay buffer... ')\n",
    "        print('\\n')\n",
    "        state = self.env.reset()\n",
    "        for i in range(replay_buffer_fill_len):\n",
    "            action = self.select_action(state, 1) # force to choose a random action\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "            self.buffer.add(state, action, reward, done, next_state)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                self.env.reset()\n",
    "\n",
    "        print('replay buffer populated with {} transitions, start training...'.format(self.buffer.count()))\n",
    "        print('\\n')\n",
    "\n",
    "        # main loop - iterate over episodes\n",
    "        for i in range(1, episodes + 1):\n",
    "            # reset the environment\n",
    "            done = False\n",
    "            state = self.env.reset()\n",
    "\n",
    "            # reset spisode reward and length\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "\n",
    "            # play until it is possible\n",
    "            while not done:\n",
    "                # synchronize target network with estimation network in required frequence\n",
    "                if (total_steps % sync_target_net_freq) == 0:\n",
    "                    self.sync_target_network()\n",
    "\n",
    "                # calculate epsilon and select greedy action\n",
    "                epsilon = self.get_epsilon(total_steps, max_epsilon_steps, epsilon_start, epsilon_final)\n",
    "                action = self.select_action(state, epsilon)\n",
    "\n",
    "                # execute action in the environment\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # store transition in replay memory\n",
    "                self.buffer.add(state, action, reward, done, next_state)\n",
    "\n",
    "                # sample random minibatch of transitions\n",
    "                s_batch, a_batch, r_batch, d_batch, next_s_batch = self.buffer.sample(batch_size)\n",
    "\n",
    "                # predict Q value using the estimation network\n",
    "                predicted_values = self.predict_q_values(s_batch)\n",
    "\n",
    "                # estimate Q value using the target network\n",
    "                q_targets = self.calculate_q_targets(next_s_batch, r_batch, d_batch)\n",
    "\n",
    "                # update weights in the estimation network\n",
    "                self.update(predicted_values, q_targets, a_batch)\n",
    "\n",
    "                # set the state for the next action selction and update counters and reward\n",
    "                state = next_state\n",
    "                total_steps += 1\n",
    "                episode_length += 1\n",
    "                episode_reward += reward\n",
    "                self.writer.add_scalar('data/reward', reward, total_steps)\n",
    "                self.writer.add_scalar('data/epsilon', epsilon, total_steps)\n",
    "\n",
    "            running_episode_reward = running_episode_reward * 0.9 + 0.1 * episode_reward\n",
    "            self.writer.add_scalar('data/episode_reward', episode_reward, i)\n",
    "            self.writer.add_scalar('data/running_episode_reward', running_episode_reward, i)\n",
    "\n",
    "\n",
    "            if (i % 30) == 0:\n",
    "                print('global step: {}'.format(total_steps))\n",
    "                print('episode: {}'.format(i))\n",
    "                print('running reward: {}'.format(round(running_episode_reward, 2)))\n",
    "                print('current epsilon: {}'.format(round(epsilon, 2)))\n",
    "                print('episode_length: {}'.format(episode_length))\n",
    "                print('episode reward: {}'.format(episode_reward))\n",
    "                curr_time = time.time()\n",
    "                print('current time: ' + time.asctime(time.localtime(curr_time)))\n",
    "                print('running for: ' + str(datetime.timedelta(seconds=curr_time - start_time)))\n",
    "                print('saving model after {} episodes...'.format(i))\n",
    "                print('\\n')\n",
    "                self.save_model_during_training(i)\n",
    "\n",
    "        print('Finish training at: '+ time.asctime(time.localtime(start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = PongAgent()\n",
    "agent.train(replay_buffer_fill_len=REPLAY_BUFFER_FILL_LEN,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            episodes=EPISODES,\n",
    "            max_epsilon_steps=MAX_EPSILON_STEPS,\n",
    "            epsilon_start=EPSILON_START,\n",
    "            epsilon_final=EPSILON_FINAL,\n",
    "            sync_target_net_freq=SYNC_TARGET_NET_FREQ)\n",
    "\n",
    "agent.close_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent = PongAgent('test')\n",
    "\n",
    "test_agent.load_model('model/current_model_90.pth')\n",
    "test_agent.play(1)\n",
    "\n",
    "test_agent.close_env()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
